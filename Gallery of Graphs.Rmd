---
title: "Gallery of Graphs"
author: "Abra Brisbin"
date: "5/27/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library(dplyr)
library(ggformula)
library(pROC)
library(randomForest)
library(caret)
library(tidyverse)
```

## The relative performance of different models/combinations of tuning parameters

### Line graphs of performance
```{r}

lambdalist <- 10^seq(-3, 1, length.out = 100)

set.seed(400)
ctrl = trainControl(method = "cv", number = 10)
fit_lasso = train(Petal.Length ~ ., 
             data = iris,
             method = "glmnet",
             na.action = na.exclude, # this data set doesn't actually have any NAs
             trControl = ctrl,
             tuneGrid = expand.grid(alpha = 1,
                                    lambda = lambdalist))

#fit_lasso

```
```{r}

gf_line(RMSE ~ lambda, data = fit_lasso$results) %>%
  gf_refine(coord_cartesian(xlim = c(0, 0.35), ylim = c(0.25, 0.5))) %>%
  gf_vline(xintercept =~ fit_lasso$finalModel$lambdaOpt, color = "red") %>%
  gf_label(0.4 ~ 0.04, label = paste("Optimal lambda \n=", round(fit_lasso$finalModel$lambdaOpt, 4)))

```

### Bar graphs of performance

```{r}
iris2 <- iris %>%
  mutate(is_virginica = factor(ifelse(Species == "virginica", 1, 0)))

ctrl = trainControl(method = "cv", number = 10)
fit_petal_length = train(is_virginica ~ Petal.Length, 
             data = iris2,
             method = "glm",
             family = "binomial",
             trControl = ctrl)
fit_sepal_length = train(is_virginica ~ Sepal.Length, 
             data = iris2,
             method = "glm",
             family = "binomial",
             trControl = ctrl)
fit_petal_width = train(is_virginica ~ Petal.Width, 
             data = iris2,
             method = "glm",
             family = "binomial",
             trControl = ctrl)
fit_sepal_width = train(is_virginica ~ Sepal.Length, 
             data = iris2,
             method = "glm",
             family = "binomial",
             trControl = ctrl)

fit_is_virginica = rbind(fit_petal_length$results, fit_sepal_length$results, fit_petal_width$results, fit_sepal_width$results)
fit_is_virginica = data.frame(Variable = c("Petal Length", "Sepal Length", "Petal Width", "Sepal Width"), fit_is_virginica)
fit_is_virginica
```

```{r}
fit_is_virginica %>%
  gf_col(Accuracy ~ Variable) %>%
  gf_labs(title = "Petal size is more informative than sepal size",
          subtitle = "When using logistic regression to predict virginica irises")
```


### ROC curves
A common mistake
```{r}
# Create a binary response
iris2 <- iris %>%
  mutate(is_virginica = ifelse(Species == "virginica", 1, 0))

fit_rf = randomForest(factor(is_virginica) ~ .-Species, data = iris2)

pred_rf = predict(fit_rf)
pred_rf = as.numeric(pred_rf)
pred_rf

roc_rf = roc(response = iris2$is_virginica, predictor = pred_rf)
plot.roc(roc_rf)

```
Correct:
```{r}
pred_rf_prob = predict(fit_rf, type = "prob")
pred_rf_prob # a matrix of predicted probabilities of class 0 vs. 1
             # we'll use the second column (the probability of class 1) as our predictors
roc_rf_prob = roc(response = iris2$is_virginica, predictor = pred_rf_prob[ ,2])
plot.roc(roc_rf_prob)

# in ggformula:
gf_line(roc_rf_prob$sensitivities ~ roc_rf_prob$specificities) %>%
  gf_refine(coord_cartesian(xlim = c(1, 0)))
```

### Heat map
```{r}
set.seed(400)
lambdalist <- 10^seq(-3, 0.5, length.out = 10)
alphalist <- seq(0, 1, by = 0.1)
  
ctrl = trainControl(method = "cv", number = 10)
fit_enet = train(Petal.Length ~ ., 
             data = iris,
             method = "glmnet",
             na.action = na.exclude, # this data set doesn't actually have any NAs
             trControl = ctrl,
             tuneGrid = expand.grid(alpha = alphalist,
                                    lambda = lambdalist))

```
```{r}
enet_mat <- fit_enet$results %>%
  select(alpha, lambda, RMSE) %>%
  pivot_wider(names_from = alpha, values_from = RMSE) 
  
enet_mat = as.matrix(enet_mat)
row.names(enet_mat) = round(enet_mat[ ,1],3)
enet_mat = enet_mat[ , -1]
```

```{r}
library(gplots)
heatmap.2(enet_mat, Rowv = FALSE, Colv = FALSE, 
          dendrogram = "none", trace = "none", 
          xlab = "alpha", ylab = "lambda")
```

## The relationship between the response and one or more of the predictors
### Scatterplot
```{r}
iris %>%
  gf_point(Petal.Length ~ Petal.Width) %>%
  gf_smooth(Petal.Length ~ Petal.Width)
```
```{r}
iris %>%
  gf_point(Petal.Length ~ Petal.Width, col =~ Species) %>%
  gf_smooth(Petal.Length ~ Petal.Width, col =~ Species, method = "lm")
```

### Boxplots
```{r}
iris %>%
  gf_boxplot(Petal.Length ~ Species)
```

### Conditional Bar Plots

Non-conditional bar plots can be difficult to interpret when one of the categories is more common than another.
```{r}
library(simexaft) # for BHS data
data(BHS)

BHS <- BHS %>%
  mutate(Heart_Disease = ifelse(CHID == 1, TRUE, FALSE),
         Smoking_Status = case_when(
           SMOKE2 == 1 ~ "current smoker", 
           SMOKE1 == 1 ~ "former smoker",
           TRUE ~ "never smoked") )

BHS %>%
  gf_bar(~ Smoking_Status, fill =~ Heart_Disease, 
         position = position_dodge())
```

```{r}
# Download conditional_bar.R from Canvas, then run the following.
source("conditional_bar.R")

conditional_bar("Smoking_Status", "Heart_Disease", BHS)
```

## The relationship between the predicted response and one or more of the predictors
### Graph predictions as a function of one or two variables, holding other variables constant
Change to BHS to use a quant and categ var
```{r}
library(simexaft)
data(BHS)
BHS2 <- BHS %>%
  mutate(Diastolic_Blood_Pressure = DBP,
         Systolic_Blood_Pressure = SBP,
         Heart_Disease = factor(ifelse(CHID == 1, TRUE, FALSE)),
         Smoking_Status = factor(case_when(
           SMOKE2 == 1 ~ "current smoker", 
           SMOKE1 == 1 ~ "former smoker",
           TRUE ~ "never smoked") )) %>%
  mutate(DRINKING = factor(DRINKING),
         ) %>%
  select(-c(PAIR, DBP, SBP, CHID, 
            SMOKE1, SMOKE2, SMOKE, 
            DTHCENS, CHDCENS, CVDCENS, 
            DIABETES, RXHYPER))
```
```{r}
set.seed(17)
ctrl = trainControl(method = "cv", number = 10)
fit_logistic = train(Heart_Disease ~ ., 
             data = BHS2,
             method = "glm",
             family = "binomial",
             trControl = ctrl)
```
Run the following code to define a new function to find the statistical mode (most frequent value) of a categorical variable.
```{r}
# Thanks to https://stackoverflow.com/questions/2547402/how-to-find-the-statistical-mode for this elegant solution.
find_mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
```

```{r}
example_data <- BHS2 %>%
  mutate(across(where(is.numeric) & c(-Systolic_Blood_Pressure), median)) %>%
  mutate(across(where(is.factor) & c(-Smoking_Status), find_mode))

```

```{r}
example_data <- example_data %>%
  mutate(example_preds = predict(fit_logistic, example_data, type = "prob")[,1])

example_data %>%
  gf_point(example_preds ~ Systolic_Blood_Pressure, col =~ Smoking_Status)
```
### Lek profiles

```{r}
library(nnet)
library(NeuralNetTools)

set.seed(100)
ctrl = trainControl(method = "cv", number = 5)
fit_sepal = train(Sepal.Width ~ .-Species,
                  data = iris,
                  method = "nnet",
                  tuneGrid = expand.grid(size = 1:5, 
                                         decay = c(0, .5, 10^(-c(1:7)))),
                  preProc = c("center", "scale"),
                  linout = TRUE,
                  maxit = 500,
                  trace = FALSE,
                  trControl = ctrl)
```
```{r}
lekprofile(fit_sepal)
```
### Partial Dependence Plot
```{r}

set.seed(800)
ctrl = trainControl(method = "cv", number = 5)
fit_rf = train(Petal.Length ~ .,
               data = iris,
             method = "rf",
             tuneGrid = expand.grid(mtry = c(1:4)),
             trControl = ctrl)

fit_rf

```
```{r}
# Download gf_partialPlot.R from Canvas, then run the following.
source("gf_partialPlot.R")
gf_partialPlot(fit_rf, iris, x.var = "Petal.Width")
```

### Decision tree
(A complexity parameter of 0 actually gives a better RMSE.  I'm using .05 to .1 here to produce a simplified graph for purposes of the video.)
```{r}
set.seed(789)
ctrl = trainControl(method = "cv", number = 5)
fit_tree = train(Petal.Length ~ ., 
             data = iris,
             method = "rpart",
             na.action = na.exclude,
             tuneGrid = expand.grid(cp = seq(.05, .1, by = .01)),
             trControl = ctrl)

fit_tree 
```

```{r}
rattle::fancyRpartPlot(fit_tree$finalModel)
```

## Why you chose the data cleaning that you did
### Transformations to reduce skew
#### Before-and-after histograms
```{r}
library(simexaft) # for BHS data
data(BHS)
BHS <- BHS %>%
  mutate(Systolic_BP = SBP,
         log_Systolic_BP = log(SBP))

BHS %>%
  gf_histogram(~Systolic_BP)
BHS %>%
  gf_histogram(~log_Systolic_BP)
```
#### Residual plots
```{r}
library(simexaft) # for BHS data
data(BHS)
BHS <- BHS %>%
  mutate(Systolic_BP = SBP,
         Diastolic_BP = DBP)

fit_lm_BP = lm(Systolic_BP ~ Diastolic_BP, data = BHS)
par(mfrow = c(1,2))
plot(fit_lm_BP)
```
### De-correlating predictors
```{r}
library(corrplot)
library(RColorBrewer) # for the Brewer color pallette
iris_numeric = select_if(iris, is.numeric)
correlations <-cor(iris_numeric, use = "pairwise.complete.obs")
corrplot(correlations, type="upper", order="hclust",
         col=rev(brewer.pal(n=8, name="RdYlBu")))
```
```{r}
library(VIM)
aggr(sleep, numbers=TRUE, sortVars=TRUE, cex.axis=.7, gap=3, 
     ylab=c("Proportion of missingness","Missingness Pattern"))
```

## Which predictors are most important
### Variable importance plot
```{r}
fit_rf = randomForest(Petal.Length ~ ., data = iris)
varImpPlot(fit_rf)
```

```{r}
imp = data.frame(Variable = rownames(fit_rf$importance),
                 IncNodePurity = fit_rf$importance)
imp %>%
  mutate(Variable = reorder(Variable, -IncNodePurity)) %>%
  gf_col(IncNodePurity ~ Variable)
```

### Garson plot
```{r}
library(nnet)
set.seed(100)
iris2 = data.frame(scale(iris[ ,1:4]))
fit_nnet = nnet(Sepal.Width ~ Petal.Length + Sepal.Length + Petal.Width, 
            data = iris2, size = 2, maxit = 400,
            linout = TRUE)
library(NeuralNetTools)
garson(fit_nnet)
```

### Linear regression:  p-values
```{r}
iris2 = data.frame(scale(iris[ ,1:4]))
fit_lm = lm(Sepal.Width ~ Petal.Length + Sepal.Length + Petal.Width, data = iris2)
results = summary(fit_lm)
lm_coef = data.frame(variable = rownames(results$coef)[-1], p_value = results$coef[-1,4])
lm_coef %>%
  mutate(variable = reorder(variable, p_value)) %>%
  gf_col(-log10(p_value) ~ variable) %>%
  gf_hline(yintercept = -log10(.05), col = "red", lty = 2) %>%
  gf_labs(title = "Sepal length and Petal length are most important",
          subtitle = "Bars above the red line are significant at the 0.05 level")
```

## These are tables, not graphs
### Confusion matrix with colors
```{r}
iris2 <- iris %>%
  mutate(is_virginica = factor(ifelse(Species == "virginica", 1, 0)))

ctrl = trainControl(method = "cv", number = 10)
fit_petal_length = train(is_virginica ~ Petal.Length, 
             data = iris2,
             method = "glm",
             family = "binomial",
             trControl = ctrl)

preds = predict(fit_petal_length)
results = data.frame(preds, truth = iris2$is_virginica)

```

```{r}

cm <- yardstick::conf_mat(results, truth, preds)

autoplot(cm, type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")
```

## Things to avoid
### Overly-complicated graphs
```{r}
library(simexaft)
data(BHS)
BHS <- BHS %>%
  mutate(Diastolic_Blood_Pressure = DBP) %>%
  mutate(Systolic_Blood_Pressure = SBP) %>%
  mutate(Heart_Disease = ifelse(CHID == 1, TRUE, FALSE)) %>%
  mutate(Smoking_Status = case_when(
    SMOKE2 == 1 ~ "current smoker", 
    SMOKE1 == 1 ~ "former smoker",
    TRUE ~ "never smoked") )

gf_point(Systolic_Blood_Pressure ~ Diastolic_Blood_Pressure, col =~ Heart_Disease, shape =~ Smoking_Status, data = BHS)
```

Raw R output
```{r}
fit_glm = glm(is_virginica ~ .-Species, data = iris2, family = "binomial")
summary(fit_glm)
```
Making a table of the relevant output
```{r}

glm_output = summary(fit_glm)
knitr::kable(round(glm_output$coefficients[ ,c(1,4)], 2), "simple")

```
